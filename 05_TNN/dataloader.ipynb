{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51223bd8",
   "metadata": {},
   "source": [
    "# Some Seq2Seq Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac17f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import torch\n",
    "import sympy as sp\n",
    "from IPython.display import display\n",
    "\n",
    "# symbols\n",
    "from sympy import symbols, sympify, exp, \\\n",
    "    cos, sin, tan, \\\n",
    "    cosh, sinh, tanh, ln, log, E, O\n",
    "x,a,b,c,d,f,g = symbols('x,a,b,c,d,f,g', real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e2e4bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(a, x):\n",
    "    print(f'{a:s}: {str(x.shape):s}')\n",
    "    \n",
    "def number_of_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# pretty print symbolic expression\n",
    "def pprint(expr):\n",
    "    display(sympify(expr))\n",
    "\n",
    "# regular expression (regex) to extract tokens\n",
    "get_tokens = re.compile('O[(]x[*][*]6[)]|[*][*]|[*]|[+]|[-]|[/]|'\\\n",
    "                        '[(]|[)]|[1-9][.]0|[0-9]|[a-zA-Z]+')\n",
    "\n",
    "# given a list of strings, extract list of tokens\n",
    "def build_vocabulary(text):\n",
    "    \n",
    "    tokens = set(['0','1','2','3','4','5','6','7','8','9'])\n",
    "\n",
    "    for ii, line in enumerate(text):\n",
    "        token  = set(get_tokens.findall(line))\n",
    "        tokens = tokens.union(token)\n",
    "\n",
    "    tokens = list(tokens)\n",
    "    tokens.sort()\n",
    "    \n",
    "    # ensure that PAD, SOS, and EOS symbols will always have codes 0, 1, 2\n",
    "    \n",
    "    tokens.insert(0, '<eos>') # end of sequence (EOS) symbol\n",
    "    tokens.insert(0, '<sos>') # start of sequence (SOS) symbol\n",
    "    tokens.insert(0, '<pad>') # padding (PAD) symbol   \n",
    "\n",
    "    # token to code map (it seems that we need to start from code 0)\n",
    "    \n",
    "    codes      = np.arange(len(tokens))\n",
    "    token2code = dict(zip(tokens, codes))\n",
    "    code2token = dict(zip(codes, tokens))\n",
    "    \n",
    "    return tokens, token2code, code2token\n",
    "\n",
    "# split string \"line\" into tokens\n",
    "\n",
    "def tokenize(line):\n",
    "    line_orig = line\n",
    "    \n",
    "    findall = get_tokens.findall\n",
    "    # 1. get a unique list of tokens from string \"line\" and sort in\n",
    "    #    decreasing length of token so that longest tokens, like \"sinh\",\n",
    "    #    are searched for before, for example, \"sin\"\n",
    "    tokens  = [(len(x), x) for x in list(set(findall(line)))]\n",
    "    tokens.sort()\n",
    "    tokens.reverse()\n",
    "\n",
    "    # 2. create a regex to search for any token in the list of tokens\n",
    "    #    (make sure that regex special symbols are not used as such)\n",
    "    tokens = [x.\\\n",
    "              replace('*', '[*]').\\\n",
    "              replace('-', '[-]').\\\n",
    "              replace('+', '[+]').\\\n",
    "              replace('(', '[(]').\\\n",
    "              replace(')', '[)]') \n",
    "                for _, x in tokens]\n",
    "\n",
    "    cmd = r'^('+'|'.join(tokens)+')'\n",
    "    cmd = re.compile(cmd)\n",
    "\n",
    "    # 3. loop through string and match a token starting at the\n",
    "    #    1st character of the string. then shorten the string\n",
    "    #    by removing the matched token and repeat until the\n",
    "    #    string as zero length.\n",
    "    max_len = len(line)\n",
    "    tokens  = []\n",
    "    j = 0\n",
    "    while (len(line) > 0) and j < max_len:\n",
    "        j += 1\n",
    "        token = cmd.findall(line)\n",
    "        if len(token) > 0:\n",
    "            tokens.append(token[0])\n",
    "            line = cmd.sub('', line)\n",
    "        else:\n",
    "            # this should never happen!\n",
    "            \n",
    "            print(\"problematic ***> \", line_orig)\n",
    "            pprint(line_orig)\n",
    "            raise ValueError(f'token not found:<<{line:s}>>')\n",
    "\n",
    "    return tokens\n",
    "        \n",
    "def stringify(codes, code2token):\n",
    "    return ''.join([code2token[int(x)] for x in codes])\n",
    "\n",
    "def text2codes(text, token2code, step=2000):\n",
    "    \n",
    "    max_len = 0    # maximum length of token sequences\n",
    "    avg_len = 0.0  # sum len_i\n",
    "    std_len = 0.0  # sum len_i**2\n",
    "    \n",
    "    codes   = []   # tokenized string mapped to integer codes\n",
    "\n",
    "    for i, line in enumerate(text):\n",
    "\n",
    "        # map source tokens to integer codes\n",
    "        cds = [token2code[t] for t in tokenize(line)]\n",
    "        codes.append(cds)\n",
    "\n",
    "        # get maximum string length (in tokens)\n",
    "        l   = len(cds)\n",
    "        if l > max_len:\n",
    "            max_len = l\n",
    "\n",
    "        avg_len += l\n",
    "        std_len += l * l\n",
    "\n",
    "        # i'm alive printout!\n",
    "        if i % step == 0:\n",
    "            print(f'\\r{i:6d}', end='')\n",
    "\n",
    "    print()\n",
    "\n",
    "    # compute average and standard deviation\n",
    "\n",
    "    avg_len /= len(text)\n",
    "    std_len /= len(text)\n",
    "    std_len  = np.sqrt(std_len - avg_len**2)\n",
    "    \n",
    "    avg_len  = int(avg_len+0.5)\n",
    "    std_len  = int(std_len+0.5)\n",
    "    \n",
    "    return codes, avg_len, std_len, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8ddab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, filename, delimit,\n",
    "                 max_seq_len=192, \n",
    "                 batch_size=128,\n",
    "                 ftrain=18/20, # fraction of data devoted to training\n",
    "                 fvalid=1/20,  # fraction of data devoted to validation\n",
    "                 ftest=1/20,   # fraction of data devoted to testing              \n",
    "                 device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):  \n",
    "        \n",
    "        max_seq_len -= 2\n",
    "        \n",
    "        # cache computational device (CPU or GPU)\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        # read and split data into a list of 2-tuples\n",
    "        \n",
    "        print('read sequences')\n",
    "        \n",
    "        text = open(filename).readlines()\n",
    "    \n",
    "        data = [x.strip().split(delimit) for x in text]\n",
    "\n",
    "        step = int(len(data)/3)\n",
    "        \n",
    "        # plot a few source/target pairs\n",
    "        for i, (src, tgt) in enumerate(data):\n",
    "            \n",
    "            if i % step == 0:\n",
    "                print(f'{i:6d} {\"-\"*83:s}')\n",
    "                pprint(src)\n",
    "                pprint(tgt)\n",
    "        print()\n",
    "    \n",
    "        # unzip into a list of sources and a list of targets\n",
    "        \n",
    "        srcs, tgts = zip(*data)\n",
    "        \n",
    "        # build source vocabulary\n",
    "        \n",
    "        src_tokens, self.src_token2code, self.src_code2token = build_vocabulary(srcs)\n",
    "        print('source vocabulary')\n",
    "        print(self.src_token2code)\n",
    "        print()\n",
    "        \n",
    "        # build target vocabulary\n",
    "        \n",
    "        tgt_tokens, self.tgt_token2code, self.tgt_code2token = build_vocabulary(tgts)\n",
    "        print('target vocabulary')\n",
    "        print(self.tgt_token2code)\n",
    "        print()\n",
    "    \n",
    "        # ---------------------------------------------------------------\n",
    "        # tokenize sequences and map to integer codes\n",
    "        # ---------------------------------------------------------------\n",
    "        print('tokenize')\n",
    "        \n",
    "        # tokenize source sequences and map to integer codes\n",
    "        \n",
    "        srcs, avg_len, std_len, max_src_len = text2codes(srcs, self.src_token2code)\n",
    "        self.SRC_AVG_SEQ_LEN = avg_len\n",
    "        self.SRC_STD_SEQ_LEN = std_len\n",
    "        src_seq_len          = min(avg_len + 3 * std_len, max_src_len, max_seq_len)\n",
    "        \n",
    "        # tokenize target sequences and map to integer codes\n",
    "        \n",
    "        tgts, avg_len, std_len, max_tgt_len = text2codes(tgts, self.tgt_token2code)\n",
    "        self.TGT_AVG_SEQ_LEN = avg_len\n",
    "        self.TGT_STD_SEQ_LEN = std_len\n",
    "        tgt_seq_len          = min(avg_len + 3 * std_len, max_tgt_len, max_seq_len)\n",
    "\n",
    "        # filter sequences\n",
    "                \n",
    "        self.srcs   = []\n",
    "        self.tgts   = []\n",
    "        for i, (src, tgt) in enumerate(zip(srcs, tgts)):\n",
    "            if len(src) > src_seq_len: continue\n",
    "            if len(tgt) > tgt_seq_len: continue\n",
    "       \n",
    "            self.srcs.append(src)\n",
    "            self.tgts.append(tgt)\n",
    "            \n",
    "        # ---------------------------------------------------------------\n",
    "        # split data into train, validation, and test sets\n",
    "        # ---------------------------------------------------------------\n",
    "        ftotal = ftrain + fvalid + ftest\n",
    "        ftrain = ftrain / ftotal\n",
    "        fvalid = fvalid / ftotal\n",
    "        ftest  = ftest  / ftotal\n",
    "        \n",
    "        ntrain = int(len(self.srcs) * ftrain)\n",
    "        nvalid = int(len(self.srcs) * fvalid)\n",
    "        ntest  = int(len(self.srcs) * ftest)  \n",
    "        \n",
    "        # ---------------------------------------------------------------\n",
    "        # cache data\n",
    "        # ---------------------------------------------------------------\n",
    "        self.train_data = [self.srcs[:ntrain], \n",
    "                           self.tgts[:ntrain]]\n",
    "\n",
    "        self.valid_data = [self.srcs[ntrain:ntrain+nvalid], \n",
    "                           self.tgts[ntrain:ntrain+nvalid]]\n",
    "\n",
    "        self.test_data  = [self.srcs[ntrain+nvalid:], \n",
    "                           self.tgts[ntrain+nvalid:]] \n",
    "        \n",
    "        # ---------------------------------------------------------------\n",
    "        # pad and delimit sequences. the codes for PAD, SOS, and EOS are\n",
    "        # the same for source and target sequences\n",
    "        # ---------------------------------------------------------------\n",
    "        PAD = self.src_token2code['<pad>']\n",
    "        SOS = self.src_token2code['<sos>']\n",
    "        EOS = self.src_token2code['<eos>']\n",
    "\n",
    "        self.PAD = PAD\n",
    "        self.SOS = SOS\n",
    "        self.EOS = EOS\n",
    "        \n",
    "        # pad training data\n",
    "        \n",
    "        print('delimit and pad training data')\n",
    "        for i, (src, tgt) in enumerate(zip(self.train_data[0], \n",
    "                                           self.train_data[1])):\n",
    "                \n",
    "            self.train_data[0][i] = [SOS] + src \\\n",
    "              + (src_seq_len-len(src))*[PAD] + [EOS]\n",
    "            \n",
    "            self.train_data[1][i] = [SOS] + tgt \\\n",
    "              + (tgt_seq_len-len(tgt))*[PAD] + [EOS]\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(f'\\r{i:6d}', end='')\n",
    "        print()\n",
    "        \n",
    "        # pad validation data\n",
    "        \n",
    "        print('delimit and pad validation data')\n",
    "        for i, (src, tgt) in enumerate(zip(self.valid_data[0], \n",
    "                                           self.valid_data[1])):\n",
    "\n",
    "            self.valid_data[0][i] = [SOS] + src \\\n",
    "              + (src_seq_len-len(src))*[PAD] + [EOS]\n",
    "            \n",
    "            self.valid_data[1][i] = [SOS] + tgt \\\n",
    "              + (tgt_seq_len-len(tgt))*[PAD] + [EOS]\n",
    "            \n",
    "            if i % 1000 == 0:\n",
    "                print(f'\\r{i:6d}', end='')\n",
    "        print()\n",
    "        \n",
    "        print('delimit test data but do not pad')\n",
    "        \n",
    "        for i, (src, tgt) in enumerate(zip(self.test_data[0], \n",
    "                                           self.test_data[1])):\n",
    "\n",
    "            self.test_data[0][i] = [SOS] + src + [EOS]\n",
    "            self.test_data[1][i] = [SOS] + tgt + [EOS]\n",
    "            if i % 1000 == 0:\n",
    "                print(f'\\r{i:6d}', end='')\n",
    "        print()\n",
    "\n",
    "        self.SRC_SEQ_LEN     = len(self.train_data[0][0])\n",
    "        self.SRC_VOCAB_SIZE  = len(self.src_token2code)\n",
    "        \n",
    "        self.TGT_SEQ_LEN     = len(self.train_data[1][0])\n",
    "        self.TGT_VOCAB_SIZE  = len(self.tgt_token2code)\n",
    "\n",
    "        print(f'avg. source sequence length: {self.SRC_AVG_SEQ_LEN:8d}')\n",
    "        print(f'std. source sequence length: {self.SRC_STD_SEQ_LEN:8d}')\n",
    "        print(f'     source sequence length: {self.SRC_SEQ_LEN:8d}')\n",
    "        print(f'     source vocabulary size: {self.SRC_VOCAB_SIZE:8d}')\n",
    "        print()\n",
    "        \n",
    "        print(f'avg. target sequence length: {self.TGT_AVG_SEQ_LEN:8d}')\n",
    "        print(f'std. target sequence length: {self.TGT_STD_SEQ_LEN:8d}')\n",
    "        print(f'     target sequence length: {self.TGT_SEQ_LEN:8d}')\n",
    "        print(f'     target vocabulary size: {self.TGT_VOCAB_SIZE:8d}')\n",
    "        print()\n",
    "\n",
    "        # convert to tensors and load onto computational device\n",
    "        # -------------------------------------------------------------\n",
    "        self.train_x = torch.tensor(self.train_data[0]).to(self.device)\n",
    "        self.train_t = torch.tensor(self.train_data[1]).to(self.device)\n",
    "    \n",
    "        self.valid_x = torch.tensor(self.valid_data[0]).to(self.device)\n",
    "        self.valid_t = torch.tensor(self.valid_data[1]).to(self.device)\n",
    "\n",
    "        self.test_x, self.test_t = self.test_data\n",
    "\n",
    "        self.train_data = [self.train_x, self.train_t]\n",
    "        self.valid_data = [self.valid_x, self.valid_t]\n",
    "        \n",
    "        print()\n",
    "        print(f'training   data: '\\\n",
    "              f'{str(self.train_x.size()):s}, '\\\n",
    "              f'{str(self.train_t.size()):s}')\n",
    "        \n",
    "        print(f'validation data: '\\\n",
    "              f'{str(self.valid_x.size()):s}, '\\\n",
    "              f'{str(self.valid_t.size()):s}')\n",
    "        \n",
    "        print(f'test data:       '\\\n",
    "              f'{len(self.test_x):d}')\n",
    "     \n",
    "        self.batch_size = batch_size\n",
    "        self.index      = 0 # for iterator\n",
    "        \n",
    "    # return a batch of data for the next step in the minimization\n",
    "    def get_batch(self, data, ii=0, batch_size=None):\n",
    "        x, t = data\n",
    "        # selects at random \"batch_size\" integers from \n",
    "        # the range [0, batch_size-1] with replacement\n",
    "        # corresponding to the row indices of the training \n",
    "        # data to be used\n",
    "        if batch_size != None:\n",
    "            rows = torch.randint(0, len(x)-1, size=(batch_size,))\n",
    "        else:\n",
    "            rows = torch.randint(0, len(x)-1, size=(self.batch_size,))\n",
    "\n",
    "        # shape: [batch_size, seq_len]\n",
    "        return x[rows], t[rows]\n",
    "    \n",
    "    def set_batch_size(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    @property    \n",
    "    def train_iterator(self):\n",
    "        self.index       = 0\n",
    "        self.data        = self.train_data\n",
    "        self.num_batches = len(self.data[0]) // self.batch_size\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def valid_iterator(self):\n",
    "        self.index       = 0\n",
    "        self.data        = self.valid_data\n",
    "        self.num_batches = len(self.data[0]) // self.batch_size\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def test_iterator(self):\n",
    "        self.index       = 0\n",
    "        self.data        = self.test_data\n",
    "        self.num_batches = len(self.data[0])\n",
    "        return self\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \n",
    "        class Batch: pass\n",
    "        \n",
    "        if self.index < self.num_batches:\n",
    "           \n",
    "            batch    = Batch()\n",
    "            src, trg = self.data\n",
    "           \n",
    "            i = self.index * self.batch_size\n",
    "            j = i + self.batch_size\n",
    "                       \n",
    "            if self.num_batches == len(src):\n",
    "                batch.src = src[self.index].unsqueeze(0)\n",
    "                batch.trg = trg[self.index].unsqueeze(0)\n",
    "            else:\n",
    "                batch.src = src[i:j]\n",
    "                batch.trg = trg[i:j]\n",
    "                        \n",
    "            self.index += 1\n",
    "            return batch\n",
    "        else:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "    \n",
    "    def data_splits(self):\n",
    "        return self.train_data, self.valid_data, self.test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7d913e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLeft:\n",
    "    '''\n",
    "    Return the amount of time left.\n",
    "    \n",
    "    timeleft = TimeLeft(N)\n",
    "    \n",
    "    N: maximum loop count\n",
    "    \n",
    "    for i in timeleft:\n",
    "        : :\n",
    "    '''\n",
    "    def __init__(self, N):\n",
    "        import time\n",
    "\n",
    "        self.N       = N        \n",
    "        self.timenow = time.time\n",
    "        self.start   = self.timenow()\n",
    "        \n",
    "    def __del__(self):\n",
    "        pass\n",
    "    \n",
    "    def __timestr(self, ii):\n",
    "        \n",
    "        # elapsed time since start\n",
    "        elapsed = self.timenow() - self.start\n",
    "        s = elapsed\n",
    "        h = int(s / 3600) \n",
    "        s = s - 3600*h\n",
    "        m = int(s / 60)\n",
    "        s = s - 60*m\n",
    "        hh= h\n",
    "        mm= m\n",
    "        ss= s\n",
    "        \n",
    "        # time/loop\n",
    "        count = ii+1\n",
    "        t = elapsed / count\n",
    "        f = 1/t\n",
    "        \n",
    "        # time left\n",
    "        s = t * (self.N - count)\n",
    "        h = int(s / 3600) \n",
    "        s = s - 3600*h\n",
    "        m = int(s / 60)\n",
    "        s =  s - 60*m\n",
    "        percent = 100 * count / self.N\n",
    "\n",
    "        return \"%10d|%6.2f%s|%2.2d:%2.2d:%2.2d/%2.2d:%2.2d:%2.2d|%6.1f it/s\" % \\\n",
    "            (count, percent, '%', hh, mm, ss, h, m, s, f)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \n",
    "        for ii in range(self.N):\n",
    "            \n",
    "            if ii < self.N-1:\n",
    "                print(f'\\r{self.__timestr(ii):s}', end='')\n",
    "            else: \n",
    "                print(f'\\r{self.__timestr(ii):s}')\n",
    "                \n",
    "            yield ii\n",
    "            \n",
    "    def __call__(self, ii, extra=''):\n",
    "        \n",
    "        if extra != '':\n",
    "            extra = \"\\x1b[1;34;48m|%s\\x1b[0m\" % extra\n",
    "           \n",
    "        if ii < self.N-1:\n",
    "            print(f'\\r{self.__timestr(ii):s}{extra:s}', end='')\n",
    "        else:\n",
    "            print(f'\\r{self.__timestr(ii):s}{extra:s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac54cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
