{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DlGeQP8NkVv"
   },
   "source": [
    "# A Tutorial on the  Transformer Neural Network\n",
    "> Alex Judge and Harrison Prosper<br>\n",
    "> Florida State University, Spring 2023 (closely follows the Annotated Transformer[1])<br>\n",
    " > __Updated__: July 4, 2023 for Terascale 2023, DESY, Hamburg, Germany\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial describes a sequence to sequence (seq2seq) neural network, called the __transformer__[1], which can be  used to translate one sequence of tokens to another. The tutorial follows closely the Annotated Transformer[2]. \n",
    "\n",
    "The seq2seq model\n",
    "consists of three parts:\n",
    "\n",
    "  1. The embedding layers: encodes the tokens and their relative positions within sequences.\n",
    "  1. The transformer layer[2]: implements the syntactic and semantic analysis.\n",
    "  1. The output layer: makes a probabilistic prediction for the next token in the output sequence given the input sequence and the current output sequence. \n",
    "\n",
    "__Tensor Convention__\n",
    "We follow the convention used in the Annotated Transformer[2] in which the batch is the first dimension in all tensors. \n",
    "\n",
    "\n",
    "## Sequence to Sequence Model \n",
    "\n",
    "### Introduction\n",
    "A transformer-based seq2seq model comprises an `encoder` and a `decoder`. The encoder embeds every token in the source sequence, $\\boldsymbol{x}$, together with its ordinal value,  in a vector space. The vectors are processed with a chain of algorithms called __attention__ and the transformed vectors together with the current target sequence, $\\boldsymbol{t}$, or current predicted sequence, $\\boldsymbol{y}$, are sent to the decoder which embeds the targets in another vector space. The target vectors are likewise processed with a chain of attention algorithms, while the target vectors and those from the encoder are processed with another attention algorithm. Finally, the decoder assigns a weight to every token in the target vocabulary. Using a greedy strategy, one chooses the next output token to be the one with the largest weight, that is, one chooses the most probable token. The model is __auto-regressive__: the predicted token is appended to the existing predicted output sequence and the model is called again with the same source and the updated output. The procedure repeats until either the maximum output sequence length is reached or the end-of-sequence (EOS) token is predicted as the most probable token.\n",
    "\n",
    "\n",
    "### Attention\n",
    "\n",
    "When we translate from one sequence of symbols to another sequence of symbols, for example from one natural language to another,  the meaning of the sequences is encoded in the symbols, their relative order, and the degree to which a given symbol is related to the other symbols. Consider the phrases \"the white house\" and \"la maison blanche\". In order to obtain a correct translation it is important for the model to encode the fact that \"la\" and \"maison\" are strongly related, while \"the\" and \"house\" are less so. It is also important for the model to encode the strong relationship between \"the\" and \"la\", between \"house\" and \"maison\", and between \"white\" and \"blanche\". That is, the model needs to *pay attention to* grammatical and semantic facts. At least that's what humans do.\n",
    "\n",
    "The need for the model to pay attention to relevant linguistic facts is the basis of the so-called [attention mechanism](https://nlp.seas.harvard.edu/annotated-transformer/). In the encoding stage, the model associates a vector to every token that tries to capture the strength of a token's relationship to other tokens. Since this association mechanism operates within the same sequence it is referred to as __self attention__. Ideally, self attention will note the fact that \"la\" and \"maison\" are strongly coupled and, ideally, that the relative positions of \"maison\" and \"blanche\" are also strongly coupled as are the relative positions of \"white\" and \"house\". In the decoding stage of the model, in addition to the self attention, this time over the target sequences, another attention mechanism should pay attention to the fact that \"the\" and \"la\", \"house\" and \"maison\", and \"white\" and \"blanche\" are strongly coupled. At a minimum, therefore, we expect a successful seq2seq model to (somehow) model self attention in both the encoding and decoding phases and source to target attention in the decoding phase. The optimal way to implement this is not known, however, the transformer model implements an attention mechanism, described next, which empirically appears to be highly effective.\n",
    "\n",
    "\n",
    "### Prediction\n",
    "As noted, the transformer is trained, and used, *auto-regressively*: given source, i.e., input, sequence $\\boldsymbol{x} = x_0, x_1,\\cdots, x_{n-1}$ or length $n$ tokens, and current output sequence  $\\boldsymbol{y}_k = y_0, y_1,\\cdots, y_{k-1}$ of length $k$ tokens, the model approximates a discrete conditional probability distribution,  over the target vocabulary of size $m$ tokens, \n",
    "\n",
    "$$p_{ij} \\equiv p(y_{ij} | \\boldsymbol{x}, \\boldsymbol{y}_k), \\quad i = 0, \\cdots, k, \\quad j = 0,\\cdots, m-1 .$$\n",
    "\n",
    "For a sequence of size $k$, there are $k^m$ possible \"sentences\". Ideally, we want to find the most probable. Alas, we have a bit of a computational problem. For a sequence of size $k=200$ tokens and a target vocabulary of size $m = 28$ tokens, there are $\\sim 2.68\\times10^{64}$ possible sentences. Even at a trillion probability calculations per second, an exhaustive search would be an utterly futile undertaking because it would take far longer to complete than the current age of the universe ($\\sim 4 \\times 10^{17}$ s)! Obviously, we need to use a heuristic strategy.\n",
    "\n",
    "The simplest strategy is the __greedy strategy__ in which we consider only the last predicted probability distribution, that is, distribution $k+1$ and choose the next token to be the most probable.  \n",
    "\n",
    "A better strategy is __beam search__ in which at each prediction stage we keep track of the $n$ \"best\" sequences so far where by best we mean the $n$ most probable sequences so far. At the end we pick the most probable sequence among the $n$.\n",
    "\n",
    "\n",
    "### References\n",
    "  1. [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "  1.  [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34921,
     "status": "ok",
     "timestamp": 1686165379916,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "zAi4I2osNkVw",
    "outputId": "1b7c1fac-eede-4984-948d-891fef7664f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running locally\n",
      "\n",
      "Computational device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "try: \n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    \n",
    "    BASE = '/content/gdrive/My Drive/transformer'\n",
    "    sys.path.append(BASE)\n",
    "    \n",
    "    print('\\nRunning in Google Colab\\n')\n",
    "    \n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Not connected to a GPU')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "    \n",
    "except:\n",
    "    BASE = '.'   \n",
    "    print('\\nRunning locally')\n",
    "\n",
    "def pathname(filename):\n",
    "    return f'{BASE:s}/{filename:s}'\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\nComputational device: {str(DEVICE):s}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1686165474275,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "vy7iBa6DNkVy"
   },
   "outputs": [],
   "source": [
    "SEED = 314159\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpE9QgwvNkVy"
   },
   "source": [
    "## Read Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "executionInfo": {
     "elapsed": 51324,
     "status": "ok",
     "timestamp": 1686165974828,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "WPWuTZDtNkVy",
    "outputId": "efff2fb7-ba09-4777-96ca-6d2fcbf86356"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read sequences\n",
      "     0 -----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(e^{3 a x} - \\frac{\\sin{\\left(c x \\right)}}{\\sinh{\\left(g x \\right)}}\\right) e^{- a x}$"
      ],
      "text/plain": [
       "(exp(3*a*x) - sin(c*x)/sinh(g*x))*exp(-a*x)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1 + x \\left(\\frac{a c}{g} + 2 a\\right) + x^{2} \\left(- \\frac{a^{2} c}{2 g} + 2 a^{2} + \\frac{c^{3}}{6 g} + \\frac{c g}{6}\\right) + x^{3} \\left(\\frac{a^{3} c}{6 g} + \\frac{4 a^{3}}{3} - \\frac{a c^{3}}{6 g} - \\frac{a c g}{6}\\right) + x^{4} \\left(- \\frac{a^{4} c}{24 g} + \\frac{2 a^{4}}{3} + \\frac{a^{2} c^{3}}{12 g} + \\frac{a^{2} c g}{12} - \\frac{c^{5}}{120 g} - \\frac{c^{3} g}{36} - \\frac{7 c g^{3}}{360}\\right) + x^{5} \\left(\\frac{a^{5} c}{120 g} + \\frac{4 a^{5}}{15} - \\frac{a^{3} c^{3}}{36 g} - \\frac{a^{3} c g}{36} + \\frac{a c^{5}}{120 g} + \\frac{a c^{3} g}{36} + \\frac{7 a c g^{3}}{360}\\right) - \\frac{c}{g} + O\\left(x^{6}\\right)$"
      ],
      "text/plain": [
       "1 + x*(a*c/g + 2*a) + x**2*(-a**2*c/(2*g) + 2*a**2 + c**3/(6*g) + c*g/6) + x**3*(a**3*c/(6*g) + 4*a**3/3 - a*c**3/(6*g) - a*c*g/6) + x**4*(-a**4*c/(24*g) + 2*a**4/3 + a**2*c**3/(12*g) + a**2*c*g/12 - c**5/(120*g) - c**3*g/36 - 7*c*g**3/360) + x**5*(a**5*c/(120*g) + 4*a**5/15 - a**3*c**3/(36*g) - a**3*c*g/36 + a*c**5/(120*g) + a*c**3*g/36 + 7*a*c*g**3/360) - c/g + O(x**6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15414 -----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle e^{d x} + \\sinh{\\left(f x \\right)} - \\tanh{\\left(d x \\right)} + \\frac{\\tanh{\\left(g x \\right)}}{\\tanh{\\left(a x \\right)}}$"
      ],
      "text/plain": [
       "exp(d*x) + sinh(f*x) - tanh(d*x) + tanh(g*x)/tanh(a*x)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{g}{a} + 1 + x^{2} \\left(\\frac{a g}{3} + \\frac{d^{2}}{2} - \\frac{g^{3}}{3 a}\\right) + x^{3} \\left(\\frac{d^{3}}{2} + \\frac{f^{3}}{6}\\right) + x^{4} \\left(- \\frac{a^{3} g}{45} - \\frac{a g^{3}}{9} + \\frac{d^{4}}{24} + \\frac{2 g^{5}}{15 a}\\right) + x^{5} \\left(- \\frac{d^{5}}{8} + \\frac{f^{5}}{120}\\right) + f x + O\\left(x^{6}\\right)$"
      ],
      "text/plain": [
       "g/a + 1 + x**2*(a*g/3 + d**2/2 - g**3/(3*a)) + x**3*(d**3/2 + f**3/6) + x**4*(-a**3*g/45 - a*g**3/9 + d**4/24 + 2*g**5/(15*a)) + x**5*(-d**5/8 + f**5/120) + f*x + O(x**6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30828 -----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle e^{2 d x} \\tan^{3}{\\left(c x \\right)}$"
      ],
      "text/plain": [
       "exp(2*d*x)*tan(c*x)**3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{5} \\left(c^{5} + 2 c^{3} d^{2}\\right) + c^{3} x^{3} + 2 c^{3} d x^{4} + O\\left(x^{6}\\right)$"
      ],
      "text/plain": [
       "x**5*(c**5 + 2*c**3*d**2) + c**3*x**3 + 2*c**3*d*x**4 + O(x**6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "source vocabulary\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '(': 3, ')': 4, '*': 5, '**': 6, '+': 7, '-': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, 'a': 20, 'b': 21, 'c': 22, 'cos': 23, 'cosh': 24, 'd': 25, 'exp': 26, 'f': 27, 'g': 28, 'sin': 29, 'sinh': 30, 'tan': 31, 'tanh': 32, 'x': 33}\n",
      "\n",
      "target vocabulary\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, '(': 3, ')': 4, '*': 5, '**': 6, '+': 7, '-': 8, '/': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, 'O(x**6)': 20, 'a': 21, 'b': 22, 'c': 23, 'd': 24, 'f': 25, 'g': 26, 'x': 27}\n",
      "\n",
      "tokenize\n",
      " 46000\n",
      " 46000\n",
      "delimit and pad training data\n",
      " 33000\n",
      "delimit and pad validation data\n",
      "  3000\n",
      "delimit test data but do not pad\n",
      "  1000\n",
      "avg. source sequence length:       29\n",
      "std. source sequence length:       10\n",
      "     source sequence length:       61\n",
      "     source vocabulary size:       34\n",
      "\n",
      "avg. target sequence length:      142\n",
      "std. target sequence length:      212\n",
      "     target sequence length:      200\n",
      "     target vocabulary size:       28\n",
      "\n",
      "\n",
      "training   data:    33318,       61,      200\n",
      "validation data:     3919,       61,      200\n",
      "test data:           1961\n"
     ]
    }
   ],
   "source": [
    "%run \"{BASE}\"/dataloader.ipynb\n",
    "\n",
    "MAX_SEQ_LEN = 200\n",
    "BATCH_SIZE  = 128\n",
    "FTRAIN      = 17\n",
    "FVALID      = 2\n",
    "FTEST       = 1\n",
    "\n",
    "filename = pathname('seq2seq_series.txt')\n",
    "delimit  = '|'\n",
    "\n",
    "dloader  = DataLoader(filename, delimit, \n",
    "                      max_seq_len=MAX_SEQ_LEN, \n",
    "                      batch_size=BATCH_SIZE, \n",
    "                      ftrain=FTRAIN, \n",
    "                      fvalid=FVALID, \n",
    "                      ftest=FTEST)\n",
    "\n",
    "train_data, valid_data, test_data = dloader.data_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rABspL0wNkVz"
   },
   "source": [
    "## The Model\n",
    "\n",
    "The transformer comprises an encoder and decoder, each of which consists of one or more processing layers.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder does the following:\n",
    " 1. Each token in the source (input) sequence is encoded as a vector $\\boldsymbol{t}$ in a space of __emb_dim__ dimensions. \n",
    " 1. The position of each token is also encoded as a vector $\\boldsymbol{p}$ in a vector space of the same dimension as $\\boldsymbol{t}$.  Both the token and position embeddings are trainable.\n",
    " 1. Each token is associated with a third vector: $\\boldsymbol{v} = \\lambda \\boldsymbol{t} + \\boldsymbol{p}$, where the scale factor $\\lambda = \\sqrt{\\text{emb_dim}}$.  \n",
    "\n",
    "The vectors $\\boldsymbol{v}$ are processed through $N$ *encoder layers*.\n",
    "\n",
    "Since the source sequences are padded so that they are all of equal length, a method is needed to ensure that the pad tokens are ignored. This is done using masks.\n",
    "The source mask, `src_mask`, has value 1 if the token in the source is *not* a `<pad>` token and 0 otherwise. The source mask is used in the encoder layers to mask the `<pad>` tokens in the multi-head attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 700,
     "status": "ok",
     "timestamp": 1686166010835,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "H3iANFmUNkVz"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size,      # vocabulary size (of source)\n",
    "                 emb_dim,         # dimension of token embedding space\n",
    "                 n_layers,        # number of encoding layers\n",
    "                 n_heads,         # number of attention heads\n",
    "                 ff_dim,          # dimension of feed-forward network\n",
    "                 dropout,         # dropout probability\n",
    "                 device,          # computational device\n",
    "                 max_len):        # maximum number of tokens/sequence\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # cache computational device\n",
    "        self.device = device\n",
    "        \n",
    "        # represent each of the 'vocab_size' possible tokens \n",
    "        # by a vector of size 'emb_dim'\n",
    "        self.tok_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        # represent the position of each token by a vector of size 'emb_dim'.\n",
    "        # 'max_len' is the maximum length of a sequence.\n",
    "        self.pos_embedding = nn.Embedding(max_len, emb_dim)\n",
    "        \n",
    "        # create 'n_layers' encoding layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(emb_dim, \n",
    "                                                  n_heads, \n",
    "                                                  ff_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "        # factor by which to scale token embedding vectors\n",
    "        self.scale  = torch.sqrt(torch.FloatTensor([emb_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src      : [batch_size, src_len]         (shape of src)\n",
    "        # src_mask : [batch_size, 1, 1, src_len]   (shape of src_mask)\n",
    "        \n",
    "        batch_size, src_len = src.shape\n",
    "  \n",
    "        # ---------------------------------------\n",
    "        # input embedding \n",
    "        # ---------------------------------------\n",
    "        src = self.tok_embedding(src)\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # ---------------------------------------\n",
    "        # position embedding\n",
    "        # ---------------------------------------\n",
    "        # create a row tensor, p, with entries [0, 1,..., src_len-1]\n",
    "        pos = torch.arange(0, src_len)\n",
    "        # pos: [src_len]\n",
    "        \n",
    "        # 1. add a dimension at position 0 (for batch size)\n",
    "        # 2. repeat p, once per row, 'batch_size' times, so that \n",
    "        #    we obtain\n",
    "        # pos = |p|\n",
    "        #       |p|\n",
    "        #        :\n",
    "        #       |p|\n",
    "        # 3. send to computational device\n",
    "        once_per_row = 1\n",
    "        pos = pos.unsqueeze(0).repeat(batch_size, once_per_row).to(self.device)\n",
    "        # pos: [batch_size, src_len]\n",
    "        \n",
    "        pos = self.pos_embedding(pos)\n",
    "        # pos: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # linearly combine token and token position embeddings\n",
    "        # could try replacing this by a feed-forward network\n",
    "        src = src * self.scale + pos\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # is this is really necessary?\n",
    "        src = self.dropout(src)\n",
    "        \n",
    "        # pass embedding vectors through encoding layers\n",
    "        # Note: the entire sequence is processed (in principle) in parallel\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask) \n",
    "            # src: [batch_size, src_len, emb_dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative (non-trained) position embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1331,
     "status": "ok",
     "timestamp": 1686166014577,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "DiblUUuXNkVz"
   },
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 emb_dim: int,           # dimension of embedding space (must be even)\n",
    "                 max_len: int,           # max_len: maximum length of sequence\n",
    "                 dropout: float):        # dropout probability\n",
    "        \n",
    "        super(PositionEmbedding, self).__init__() # initialize parent class\n",
    "        \n",
    "        # den = 10000^(-2j / d), j = 0, 1, 2,... emb_size / 2\n",
    "        den = torch.exp(-torch.arange(0, emb_dim, 2) * math.log(10000) / emb_dim)\n",
    "        # [emb_dim/2]    #  a row-wise vector\n",
    "        \n",
    "        pos = torch.arange(0, max_len).reshape(max_len, 1)\n",
    "        # [max_len, 1]   # a column-wise vector\n",
    "        \n",
    "        # compute outer product of pos and den\n",
    "        # x_mn = pos_m * den_n\n",
    "        x   = pos * den\n",
    "        # [max_len, emb_dim/2]\n",
    "        \n",
    "        pos_encoding = torch.zeros((max_len, emb_dim))\n",
    "        # [max_len, emb_dim]\n",
    "        \n",
    "        # set every other column starting at column 0\n",
    "        pos_encoding[:, 0::2] = torch.sin(x)\n",
    "        \n",
    "        # set every other column starting at column 1\n",
    "        pos_encoding[:, 1::2] = torch.cos(x)\n",
    "        \n",
    "        # use unsqueeze 0 to place a third dimension (batch) in position 0\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        # [1, max_len, emb_dim]\n",
    "               \n",
    "        # registering a tensor as a buffer tells PyTorch that the tensor\n",
    "        # is not to be changed during optimization.\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "\n",
    "    def forward(self, seq_len: int):\n",
    "        # make sure sequence length of position encoding matches\n",
    "        # that of token sequences.\n",
    "        p = self.pos_encoding[:, :seq_len, :]\n",
    "        #p: [1, seq_len, emb_dim]\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kE4K943iNkV0"
   },
   "source": [
    "### Encoder Layer\n",
    "\n",
    " 1. Pass the source tensor and its mask to the *multi-head attention layer*.\n",
    " 1. Apply a residual connection and [Layer Normalization](https://arxiv.org/abs/1607.06450). \n",
    " 1. Apply a linear layer.\n",
    " 1. Apply a residual connection and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686166016054,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "hGmYBW5PNkV1"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 emb_dim, \n",
    "                 n_heads, \n",
    "                 ff_dim,  \n",
    "                 dropout, \n",
    "                 device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention       = MultiHeadAttention(emb_dim, \n",
    "                                                       n_heads, \n",
    "                                                       dropout, \n",
    "                                                       device)\n",
    "        self.self_attention_norm  = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.feedforward          = Feedforward(emb_dim, ff_dim, dropout)\n",
    "        self.feedforward_norm     = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src      : [batch_size, src_len, emb_dim]\n",
    "        # src_mask : [batch_size, 1, 1, src_len] \n",
    "          \n",
    "        # ------------------------------------------\n",
    "        # self attention over embedded source\n",
    "        # ------------------------------------------\n",
    "        # distinguish between src and src_ as the \n",
    "        # former is needed later for a residual connection\n",
    "        src_ = self.self_attention(src, src, src, src_mask)\n",
    "        # src_: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        # is this useful?\n",
    "        src_ = self.dropout(src_)\n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # add residual connection then layer norm.\n",
    "        # ------------------------------------------\n",
    "        # distinguish between src and src+src_ as the\n",
    "        # former is later needed for a residual connection\n",
    "        src  = self.self_attention_norm(src + src_)\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        src_ = self.feedforward(src)\n",
    "        # src_: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        src_ = self.dropout(src_)\n",
    "\n",
    "        # add residual connection and layer norm\n",
    "        src  = self.feedforward_norm(src + src_)\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTKA5UTANkV1"
   },
   "source": [
    "### Mutli-Head Attention Layer\n",
    "\n",
    "\n",
    "Attention in the transformer model is defined by the matrix expression\n",
    "\n",
    "\\begin{align}\n",
    "    \\textrm{Attention}(Q, K, V) & = \\textrm{Softmax}\\left(\\frac{Q K^T}{\\sqrt{d}} \\right) V,\n",
    "\\end{align}\n",
    "\n",
    "where $Q$ is called the `query`, $K$ the `key`, $V$ the `value`, and $d =$ __emb_dim__ is the dimension of the vectors that represent the tokens. In practice, the vectors are split into __n_heads__ pieces each of size __head_dim__ $= \\text{emb_dim} / \\text{n_heads}$. __n_heads__ is the number of so-called __attention heads__. (It is stated that each `head` can pay attention to different aspects of a sequence. However, at our current level of understanding of how functions with millions of parameters truly work, such statements should be taken with a liberal pinch of salt.)\n",
    "In self attention, the query, key, and value tensors are derived from the same tensor via separate linear transformations of that tensor (see Attention Algorithm below). The coefficients of the linear functions are free parameters to be set by the training algorithm.  The number of rows in $Q$, $K$, and $V$, namely, __query_len__,  __key_len__, and __value_len__, respectively, is equal to the sequence length __seq_len__. For target/source attention, the query is a linear function of the target tensor while the key and value tensors are linear functions of the source tensor, where, again, the coefficients are free parameters to be fitted during training.\n",
    "\n",
    "We first describe the attention mechanism mathematically and then follow with an algorithmic description that closely follows \n",
    "the description in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/). It is to be understood that every operation described below is performed for a batch of sequences. Therefore, when we refer to a matrix we really mean a batch of matrices. \n",
    "\n",
    "First consider the matrix product $Q K^T$ in component form, where summation over repeated indices (the Einstein convention) is implied,\n",
    "\n",
    "\\begin{align}\n",
    "A_{qk} \n",
    "& = Q_{q h} \\, [K^T]_{hk}, \\nonumber\\\\\n",
    "& \\quad q=1,\\cdots, \\text{query_len}, \\,\\, h = 1, \\cdots, \\text{head_dim}, \\,\\, k = 1, \\cdots, \\text{key_len} .\n",
    "\\end{align}\n",
    "\n",
    "When the matrix $A$ is scaled and a softmax function is applied elementwise along the key length dimension (here, horizontally) the result is another matrix $W$ whose row elements, by construction, sum to unity. The matrix $W$ is then multiplied by $V$ to yield\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Attention}_{qh}  \n",
    "    & = W_{qk} V_{kh}. \n",
    "\\end{align}\n",
    "\n",
    "Since tokens are represented by vectors, it is instructive to think of the attention computation geometrically.   Each row, $i$, of $Q$, $K$, and $V$ can be regarded as the vectors $\\boldsymbol{q}_i$, $\\boldsymbol{k}_i$, and $\\boldsymbol{v}_i$, respectively, associated with token $i$. Consider a sequence with __seq_len__ = 2. We can write $Q$, $K$, and $V$ as\n",
    "\n",
    "\\begin{align}\n",
    "Q & = \\left[\\begin{matrix} \\boldsymbol{q}_1 \\\\ \\boldsymbol{q}_2 \\end{matrix}\\right], \\\\\n",
    "K & = \\left[\\begin{matrix} \\boldsymbol{k}_1 \\\\ \\boldsymbol{k}_2 \\end{matrix}\\right], \\text{ and} \\\\\n",
    "V & = \\left[\\begin{matrix} \\boldsymbol{v}_1 \\\\ \\boldsymbol{v}_2 \\end{matrix}\\right] ,\n",
    "\\end{align}\n",
    "\n",
    "and $A = Q K^T$ as the outer product matrix\n",
    "\n",
    "\\begin{align}\n",
    "A & = \\left[\\begin{matrix} \\boldsymbol{q}_1 \\\\ \\boldsymbol{q}_2 \\end{matrix}\\right] \n",
    "\\left[\\begin{matrix} \\boldsymbol{k}_1 & \\boldsymbol{k}_2 \\end{matrix}\\right] ,\n",
    "\\nonumber\\\\\n",
    "& = \\left[\n",
    "\\begin{matrix} \n",
    "\\boldsymbol{q}_1\\cdot\\boldsymbol{k}_1 & \\boldsymbol{q}_1\\cdot \\boldsymbol{k}_2 \\\\ \n",
    "\\boldsymbol{q}_2\\cdot\\boldsymbol{k}_1 & \\boldsymbol{q}_2\\cdot \\boldsymbol{k}_2\n",
    "\\end{matrix}\n",
    "\\right] .\n",
    "\\end{align}\n",
    "\n",
    "The matrix $A$ can be interpreted as a measure of the degree to which the $\\boldsymbol{q}$ and $\\boldsymbol{k}$ vectors are aligned. Presumably, the more aligned the two vectors the stronger the relationship between the  tokens they represent. Because of the use of the dot product, the degree of alignment depends both on the angle between the vectors as well as on their magnitudes. Consequently, two vectors can be more strongly aligned than a vector's alignment with itself! \n",
    "\n",
    "After the scaling and softmax operations on $A$, tokens 1 and 2 become associated with vectors $\\boldsymbol{w}_1 =  (w_{11}, w_{12})$ and $\\boldsymbol{w}_2 =  (w_{21}, w_{22})$, respectively, where\n",
    "\n",
    "\\begin{align}\n",
    "    w_{ij} & = \\frac{\\exp\\left(\\boldsymbol{q}_i \\cdot \\boldsymbol{k}_j \\, / \\, \\sqrt{d}\\right)}\n",
    "    {\\sum_{k = 1}^2 \\exp\\left(\\boldsymbol{q}_i \\cdot \\boldsymbol{k}_k \\, / \\, \\sqrt{d}\\right)} .\n",
    "\\end{align}\n",
    "\n",
    "These vectors lie in the line segment $[\\boldsymbol{p}_1, \\boldsymbol{p}_2]$ depicted in the figure below. The line segment is a simplex (specifically, a 1-simplex) that is embedded in a vector space of dimension __seq_len__.  In this vector space, tokens 1 and 2 are represented by the orthogonal unit vectors $\\boldsymbol{u}_1$ and $\\boldsymbol{u}_2$, respectively. For a sequence of length $n$, the vectors $\\boldsymbol{w}_i$, $i = 1,\\cdots, n$ lie in the $(n-1)$-simplex and, again, each coordinate unit vector $\\boldsymbol{u}_i$ represents a token.  \n",
    "\n",
    "![simplex](simplex.png)\n",
    "\n",
    "In the figure above, notice that vector $\\boldsymbol{w}_2$ is closer to $\\boldsymbol{u}_1$ than it is to $\\boldsymbol{u}_2$ indicating that token 2 is more strongly aligned with token 1 than token 2 is aligned with itself, while the converse is true of token 1. The attention vector for token, $i$, in the transformer model is simply the weighted average \n",
    "\n",
    "\\begin{align}\n",
    "    \\text{Attention}_i & = w_{i1}  \\boldsymbol{v}_1 + w_{i2} \\boldsymbol{v}_2\n",
    "\\end{align}\n",
    "\n",
    "of the so-called value vectors $\\boldsymbol{v}_1$ and $\\boldsymbol{v}_2$.  The upshot of this construction is that the attention vectors can be viewed as linear functions of the source or target tensors with coefficients that depend non-linearly on the source or target. Consequently, the attention adapts to the sequences as, presumably, it should.\n",
    "\n",
    "\n",
    "### Attention Algorithm\n",
    "\n",
    "Now we describe the transformer attention mechanism algorithmically, again following closely the description in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/), but with some notational changes.\n",
    "\n",
    "#### Step 1\n",
    "As noted, the attention mechanism starts with three tensors, $Q_\\text{in}$, $K_\\text{in}$, and $V_\\text{in}$, of shapes __[batch_size, query_len, emb_dim]__, __[batch_size, key_len, emb_dim]__, and __[batch_size, value_len, emb_dim]__, respectively, with __value_len=key_len__. (emb_dim is called hid_dim in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)).  For self attention, $Q_\\text{in}$, $K_\\text{in}$, and $V_\\text{in}$ are the same tensor, while for target/source attention $Q_\\text{in}$ is associated with the target tensor and $K_\\text{in}$ and $V_\\text{in}$ with the source tensor.\n",
    "\n",
    "Three trainable linear layers, $f_Q$, $f_K$, $f_V$, are defined, each of shape __[emb_dim, emb_dim]__, which yield the so-called `query`, `key`, and `value` tensors\n",
    "\n",
    "\\begin{align}\n",
    "    Q & = f_Q(\\boldsymbol{Q_\\text{in}}),\\\\\n",
    "    K & = f_K(\\boldsymbol{K_\\text{in}}), \\text{ and}\\\\\n",
    "    V & = f_V(\\boldsymbol{V_\\text{in}}).\n",
    "\\end{align}\n",
    "\n",
    "Each tensor $Q$, $K$, and $V$ is the same shape as $Q_\\text{in}$, $K_\\text{in}$, and $V_\\text{in}$, respectively. \n",
    "\n",
    "\n",
    "#### Step 2\n",
    "Tensors $Q$, $K$, and $V$ are reshaped by first splitting the embedding dimension, __emb_dim__, into __n_heads__ blocks of size __head_dim = emb_dim / n_heads__ so that their shapes become __[batch_size, -1, n_heads, head_dim]__, where the __-1__ pertains to __query_len__, __key_len__, or __value_len__, whose value is determined at runtime.\n",
    "\n",
    "#### Step 3\n",
    "Dimensions 1 and 2 of the tensors $Q$, $K$, and $V$ are permuted (`Tensor.permute(0, 2, 1, 3)`) so that we now have __[batch_size, n_heads, -1, head_dim]__. Tensor $K$ is further permuted (`Tensor.permute(0, 1, 3, 2)`) to shape __[batch_size, n_heads, head_dim, -1]__ so that it represents $K^T$.\n",
    "\n",
    "#### Step 4\n",
    "Tensor $A = Q K^T$ is computed using `torch.matmul(Q, K^T)`, scaled by $1 \\, / \\, \\sqrt{d}$, and a softmax is applied to the last dimension of $A$, that is, the key/value length dimension, yielding the tensor $W$ of shape __[batch_size, n_heads, query_len, key_len]__.\n",
    "\n",
    "\n",
    "#### Step 5\n",
    "$\\text{Attention} = W V$ is computed, yielding a tensor of shape \n",
    "__[batch_size, n_heads, query_len, head_dim]__.\n",
    "\n",
    "#### Step 6\n",
    "The __n_heads__ and __query_len__ dimensions of `Attention` are transposed (`Tensor.permute(0, 2, 1, 3)`) to shape __[batch_size, query_len, n_heads, head_dim]__ and forced to be contiguous in memory (`contiguous()`).\n",
    "\n",
    "#### Step 7\n",
    "The __n_heads__ and __head_dim__ are concatenated using `Attention.view(batch_size, -1, emb_dim)` to merge the attention heads into a single `MultiHeadAttention` tensor.\n",
    "\n",
    "#### Step 8\n",
    "Finally, the merged `MultiHeadAttention` tensor is pushed through a trainable linear layer of shape __[emb_dim, emb_dim]__ to output a tensor of shape __[batch_size, -1, emb_dim]__, where __-1__ is the sequence length.\n",
    "\n",
    "\n",
    "### Comments\n",
    "It is claimed that the  algorithm above captures the notion of \"paying attention to\" token-token associations both within the same sequence and across sequences and that each attention head \"pays attention to\" a different aspect of the sequences. All such claims should be taken with a pinch of salt for at least two reasons.\n",
    "First, it is far from clear that this computation aligns with our intuitive understanding of  that notion and, second, the computation is nested through multiple attention layers. Therefore, whatever the attention layers are doing, it is distributed over multiple layers in a highly non-linear way. \n",
    "\n",
    "It is, however, undeniable that the transformer has yielded amazing results. Therefore, we are forced to concede that, in practice,  whatever is going on in the attention layers the algorithm works wonders!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686166016625,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "NUD-6rRKNkV1"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, n_heads, dropout, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # emb_dim must be a multiple of n_heads\n",
    "        assert emb_dim % n_heads == 0\n",
    "        \n",
    "        self.emb_dim  = emb_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        \n",
    "        self.linear_Q = nn.Linear(emb_dim, emb_dim)\n",
    "        self.linear_K = nn.Linear(emb_dim, emb_dim)\n",
    "        self.linear_V = nn.Linear(emb_dim, emb_dim)\n",
    "        self.linear_O = nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        # query  : [batch_size, query_len, emb_dim]\n",
    "        # key    : [batch_size, key_len,   emb_dim]\n",
    "        # value  : [batch_size, value_len, emb_dim]\n",
    "        \n",
    "        batch_size, _, emb_dim = query.shape\n",
    "        assert emb_dim == self.emb_dim\n",
    "        \n",
    "        Q = self.linear_Q(query)\n",
    "        # Q: [batch_size, query_len, emb_dim]\n",
    "        \n",
    "        K = self.linear_K(key)\n",
    "        # K: [batch_size, key len,   emb_dim]\n",
    "        \n",
    "        V = self.linear_V(value)\n",
    "        # V: [batch_size, value_len, emb_dim]\n",
    "        \n",
    "        # split vectors of size emb_dim into 'n_heads' vectors of size 'head_dim'\n",
    "        # and then permute dimensions 1 and 2\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
    "        \n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # K: [batch_size, n_heads, key_len,   head_dim]        \n",
    "        \n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # V: [batch_size, n_heads, value_len, head_dim]\n",
    "          \n",
    "        # transpose K (by permuting key_len and head_dim), then\n",
    "        # compute QK^T/scale\n",
    "        A = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        # A: [batch_size, n_heads, query_len, key_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            A = A.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # apply softmax to the last dimension (i.e, to key len)\n",
    "        # WARNING: W is referred to as 'attention' in Annotated Transformer!\n",
    "        W = torch.softmax(A, dim=-1)     \n",
    "        # W: [batch_size, n_heads, query_len, key_len]\n",
    "        \n",
    "        # not sure why dropout is useful here\n",
    "        W = self.dropout(W)\n",
    "        \n",
    "        # compute attention: (QK^T/scale)V\n",
    "        attention  = torch.matmul(W, V)\n",
    "        # attention: [batch_size, n_heads, query_len, head_dim]\n",
    "        \n",
    "        # permute n heads and query len and make sure the tensor \n",
    "        # is contiguous in memory...\n",
    "        attention = attention.permute(0, 2, 1, 3).contiguous()\n",
    "        # attention: [batch_size, query_len, n_heads, head_dim]\n",
    "        \n",
    "        # ... and concatenate the n heads into a single multi-head \n",
    "        # attention tensor\n",
    "        attention = attention.view(batch_size, -1, self.emb_dim)\n",
    "        # attention: [batch_size, query_len, emb_dim]\n",
    "        \n",
    "        output    = self.linear_O(attention)\n",
    "        # output: [batch_size, query_len, emb_dim]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by ChatGPT v3.5!\n",
    "def group_sort(input):\n",
    "    \"\"\"\n",
    "    Sorts the input tensor into groups of size 2 and sorts each group independently.\n",
    "\n",
    "    Args:\n",
    "        input (torch.Tensor): The input tensor to be sorted.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The sorted tensor, with elements grouped and sorted in ascending order.\n",
    "    \"\"\"\n",
    "    # Reshape the input tensor into groups of size 2\n",
    "    grouped_tensor = input.view(-1, 2)\n",
    "\n",
    "    # Sort each group individually using torch.sort\n",
    "    sorted_groups, _ = torch.sort(grouped_tensor)\n",
    "    \n",
    "    # Flatten the sorted groups tensor\n",
    "    sorted_tensor = sorted_groups.reshape(input.shape)\n",
    "\n",
    "    return sorted_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oacePmrSNkV2"
   },
   "source": [
    "### Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1324,
     "status": "ok",
     "timestamp": 1686166021797,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Uzlh65gaNkV2"
   },
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dim, ff_dim, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(emb_dim, ff_dim)\n",
    "        \n",
    "        self.linear_2 = nn.Linear(ff_dim, emb_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        x = self.linear_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # x: [batch_size, seq_len, ff_dim]\n",
    "        \n",
    "        x = self.linear_2(x)\n",
    "        # x: [batch_size, seq_len, emb_dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlidARf5NkV2"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder takes the encoded representation of the source sequence and the target sequence, or current predicted output sequence, and predicts, probabilistically, the next output token. \n",
    "\n",
    "The decoder has two multi-head attention layers: a *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value.\n",
    "\n",
    "__Note__: In PyTorch, the softmax operation is contained within the loss function, so the decoder does not have a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686166025870,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "dMQz3fxONkV3"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size,   # size of target vocabulary\n",
    "                 emb_dim, \n",
    "                 n_layers, \n",
    "                 n_heads, \n",
    "                 ff_dim, \n",
    "                 dropout, \n",
    "                 device,\n",
    "                 max_len):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(max_len, emb_dim)\n",
    "        \n",
    "        self.layers  = nn.ModuleList([DecoderLayer(emb_dim, \n",
    "                                                   n_heads, \n",
    "                                                   ff_dim, \n",
    "                                                   dropout, \n",
    "                                                   device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.linear  = nn.Linear(emb_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale   = torch.sqrt(torch.FloatTensor([emb_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, src, trg_mask, src_mask):\n",
    "        # trg      : [batch_size, trg_len]\n",
    "        # src      : [batch_size, src_len, emb_dim]\n",
    "        # trg_mask : [batch_size, 1, trg_len, trg_len]\n",
    "        # src_mask : [batch_size, 1, 1, src_len]\n",
    "                \n",
    "        batch_size, trg_len = trg.shape\n",
    "        \n",
    "        # see Encoder for comments\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)                  \n",
    "        # pos: [batch_size, trg_len]\n",
    "            \n",
    "        trg = self.tok_embedding(trg) * self.scale + self.pos_embedding(pos)\n",
    "        # trg: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg = self.dropout(trg)\n",
    "        \n",
    "        # send the same input source to every decoding layer, with the\n",
    "        # input target entering the first layer and its output target \n",
    "        # entering the second layer etc.\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, src, trg_mask, src_mask)\n",
    "            # trg: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        output = self.linear(trg)\n",
    "        # output: [batch_size, trg_len, vocab_size]\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBhZAhu4NkV3"
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "The decoder layer has two multi-head attention layers, `self_attention` and `attention`. The former applies the attention algorithm to the target sequences, while the latter applies the algorithm between the target and source sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1686166028973,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "JgYbJN_6NkV3"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 emb_dim, \n",
    "                 n_heads, \n",
    "                 ff_dim, \n",
    "                 dropout, \n",
    "                 device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention      = MultiHeadAttention(emb_dim, n_heads, dropout, device)\n",
    "        self.self_attention_norm = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.attention           = MultiHeadAttention(emb_dim, n_heads, dropout, device)\n",
    "        self.attention_norm      = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.feedforward         = Feedforward(emb_dim, ff_dim, dropout)\n",
    "        self.feedforward_norm    = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, src, trg_mask, src_mask):\n",
    "        # trg      : [batch size, trg len, emb dim]\n",
    "        # src      : [batch size, src len, emb dim]\n",
    "        # trg_mask : [batch size, 1, trg len, trg len]\n",
    "        # src_mask : [batch size, 1, 1, src len]\n",
    "        \n",
    "        # compute attention over embedded target sequences.\n",
    "        # distinguish between trg and trg_, since the former \n",
    "        # is needed later for residual connections.\n",
    "        trg_ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        # trg_: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg_ = self.dropout(trg_)\n",
    "        \n",
    "        # residual connection and layer norm\n",
    "        # ?? trg has not had the target mask applied, so the\n",
    "        # residual connection must surely dilute the effect of\n",
    "        # of the masked tensor trg_ ??\n",
    "        trg  = self.self_attention_norm(trg + trg_)\n",
    "        # trg: [batch_size, trg_len, emb_dim]\n",
    "            \n",
    "        # target/source attention\n",
    "        trg_ = self.attention(trg, src, src, src_mask)\n",
    "        # trg_: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg_ = self.dropout(trg_)\n",
    "        \n",
    "        # residual connection and layer norm\n",
    "        trg  = self.attention_norm(trg + trg_)      \n",
    "        # trg: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg_ = self.feedforward(trg)\n",
    "        # trg_: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        trg = self.dropout(trg)\n",
    "        \n",
    "        # residual and layer norm\n",
    "        trg  = self.feedforward_norm(trg + trg_)\n",
    "        # trg: [batch_size, trg_len, emb_dim]\n",
    "        \n",
    "        return trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B7SgrOYNkV3"
   },
   "source": [
    "### Seq2Seq\n",
    "\n",
    "The `Seq2Seq` model encapsulates the encoder and decoder and handles the creation of the source and target masks.\n",
    "\n",
    "The source mask, as described above, masks out `<pad>` tokens: the mask is 1 where the token is *not* a `<pad>` token and 0 if it is. The mask is then unsqueezed so it can be correctly broadcast to tensors of shape **_[batch_size, n_heads, seq_len, seq_len]_**, which appear in the multi-head attention mechanism.\n",
    "\n",
    "The target mask also includes a mask for the `<pad>` tokens. Next, we create a *subsequent* mask, `trg_sub_mask`, using `torch.tril`. This creates a diagonal matrix where the elements above the diagonal are zero and the elements below the diagonal are one. For example, for a target comprising 5 tokens the `trg_sub_mask` will look like this:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "When the mask is applied to a target sequence (a column vector), each token in the target sequence, which corresponds to a row, is associated with the target tokens with non-zero column entries in that row. For example, the first token of the target sequence has the mask **_[1, 0, 0, 0, 0]_**. Therefore, that token is associated with the first target token. The second token of the target sequence has the mask **_[1, 1, 0, 0, 0]_**; therefore, that token is associated with both the first and second target tokens. \n",
    "\n",
    "The pad mask causes the model to ignore `<pad>` tokens, which makes sense since the latter contain no useful information, while for a given token in a target sequence, the subsequent mask causes the model to ignore tokens that are subsequent to a given target sequence token. This also makes sense since the goal is to have the model predict the next token given a sequence of tokens. If all target tokens were given for all tokens in the target sequence, the model would never learn to predict the next token. \n",
    "\n",
    "The target mask is the logical and of the pad and subsequent masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1686166033948,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "Ifek5BPqNkV4"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder, \n",
    "                 src_pad, \n",
    "                 trg_pad, \n",
    "                 device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad = src_pad\n",
    "        self.trg_pad = trg_pad\n",
    "        self.device  = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        # src: [batch_size, src_len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad).unsqueeze(1).unsqueeze(2)\n",
    "        # [batch_size, 1, 1, src_len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg: [batch size, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "            \n",
    "        trg_pad_mask = (trg != self.trg_pad).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), \n",
    "                                             device=self.device)).bool()\n",
    "        # trg_sub_mask: [trg_len, trg_len]\n",
    "            \n",
    "        # logical AND of the two masks\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        \n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
    "        \n",
    "        src      = self.encoder(src, src_mask)\n",
    "        # src: [batch_size, src_len, emb_dim]\n",
    "                \n",
    "        # the decoder will encode the target sequences \n",
    "        # before applying the attention layers.\n",
    "        output   = self.decoder(trg, src, trg_mask, src_mask)\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haNBPOjUNkV4"
   },
   "source": [
    "## Training the Seq2Seq Model\n",
    "\n",
    "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but is able to be run on a single NVIDIA V100 GPU in an hour or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB4xmAIVNkV4"
   },
   "source": [
    "### Training Loop\n",
    "\n",
    "We want the model to predict the `<eos>` token, thereby terminating its predicted sequence. Therefore, we slice off the `<eos>` token from the end of the target:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3],\n",
    "\\end{align*}$$\n",
    "\n",
    "where the $x_i$ denotes target sequence tokens other than `<sos>` and `<eos>`. The sliced targets are fed into the model to get a predicted sequence. If all goes well, the model should predict the `<eos>` token, thereby terminating the predicted sequence:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos],\n",
    "\\end{align*}$$\n",
    "\n",
    "where the\n",
    "$y_i$ are the predicted target sequence tokens. The loss is computed using the original `trg` tensor with the `<sos>` token sliced off:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1686169911360,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "HTJs53_3NkV4"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader,\n",
    "          niterations, dictfile, \n",
    "          batch_size, pad_code,\n",
    "          traces, \n",
    "          lossfile=pathname('losses.txt'),\n",
    "          valid_size=256,\n",
    "          step=100):\n",
    "    \n",
    "    train_data, valid_data, _ = dataloader.data_splits()\n",
    "    dataloader.set_batch_size(batch_size)\n",
    "    \n",
    "    xx, yy_t, yy_v = traces\n",
    "    \n",
    "    v_min = 1.e20 # minimum validation loss\n",
    "    \n",
    "    def compute_loss(x, t):\n",
    "        # x: [batch_size, src_seq_len]\n",
    "        # t: [batch_size, trg_seq_len]\n",
    "       \n",
    "        # slice off EOS token from all targets\n",
    "        y = model(x, t[:,:-1])\n",
    "        # [batch_size, trg_seq_len, trg_vocab_size]\n",
    "        \n",
    "        trg_vocab_size = y.shape[-1]\n",
    "        \n",
    "        y_out = y.reshape(-1, trg_vocab_size)\n",
    "        # [batch_size * tgt_seq_len, tgt_vocab_size]\n",
    "        \n",
    "        # slice of SOS token from targets\n",
    "        t_out = t[:, 1:].reshape(-1)\n",
    "        # [batch_size * tgt_seq_len]\n",
    "        \n",
    "        loss  = loss_fn(y_out, t_out).mean()\n",
    "\n",
    "        return loss\n",
    "  \n",
    "    def validate(ii):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():  # no need to compute gradients wrt. to x, t\n",
    "                \n",
    "            x, t   = dataloader.get_batch(train_data, ii, batch_size=valid_size)\n",
    "            t_loss = compute_loss(x, t).item()\n",
    "                \n",
    "            x, t   = dataloader.get_batch(valid_data, ii, batch_size=valid_size)\n",
    "            v_loss = compute_loss(x, t).item()\n",
    "            if len(xx) < 1:\n",
    "                xx.append(0)\n",
    "            else:\n",
    "                xx.append(xx[-1]+step)\n",
    "            yy_t.append(t_loss)\n",
    "            yy_v.append(v_loss)\n",
    "            \n",
    "        return t_loss, v_loss\n",
    "    \n",
    "    timeleft = TimeLeft(niterations)\n",
    "    \n",
    "    for ii in range(niterations):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        src, tgt = dataloader.get_batch(train_data, ii)\n",
    "        \n",
    "        loss     = compute_loss(src, tgt)\n",
    "\n",
    "        optimizer.zero_grad()     # zero gradients\n",
    "        \n",
    "        loss.backward()           # compute gradients\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        optimizer.step()          # make a single step in average loss\n",
    "           \n",
    "        if (ii % step == 0) or (ii >= niterations-1):\n",
    "        \n",
    "            t_loss, v_loss = validate(ii)\n",
    "            \n",
    "            line = f'{t_loss:12.6f}|{v_loss:12.6f}|{np.exp(v_loss):12.6f}'\n",
    "\n",
    "            open(lossfile, 'a').write(f'{ii:8d} {t_loss:12.6f} {v_loss:12.6f}\\n')\n",
    "            \n",
    "            if v_loss < v_min:\n",
    "                v_min = v_loss\n",
    "                # save best model so far\n",
    "                torch.save(model.state_dict(), dictfile) \n",
    "\n",
    "            timeleft(ii, line)\n",
    "\n",
    "    print()\n",
    "\n",
    "    torch.save(model.state_dict(), 'model.pt')\n",
    "\n",
    "    return xx, yy_t, yy_v\n",
    "\n",
    "def train_by_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        print(f'\\r\\tbatch: {i:10d}', end='')\n",
    "        \n",
    "        src = batch.src\n",
    "        # [batch_size, src_len]\n",
    "        \n",
    "        trg = batch.trg\n",
    "        # [batch_size, trg_len]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # slice off EOS token from targets\n",
    "        output = model(src, trg[:,:-1])\n",
    "        # [batch_size, trg_len - 1, output_dim]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        # [batch_size * (trg_len - 1), output_dim]\n",
    "        \n",
    "        # slice off SOS token\n",
    "        trg = trg[:,1:].contiguous().view(-1)    \n",
    "        # [batch size * (trg len - 1)]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print() \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_by_epoch(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            output = model(src, trg[:,:-1])\n",
    "            # [batch size, trg len - 1, output dim]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            # [batch size * (trg len - 1), output dim]\n",
    "            \n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            # [batch size * (trg len - 1)]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1686166041708,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "jFGiIcinNkV4"
   },
   "outputs": [],
   "source": [
    "def plot_average_loss(traces, ftsize=18, filename=pathname('fig_loss.png')):\n",
    "    \n",
    "    xx, yy_t, yy_v = traces\n",
    "    \n",
    "    # create an empty figure\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # add a subplot to it\n",
    "    nrows, ncols, index = 1,1,1\n",
    "    ax  = fig.add_subplot(nrows,ncols,index)\n",
    "\n",
    "    ax.set_title(\"Average loss\")\n",
    "    \n",
    "    ax.plot(xx, yy_t, 'b', lw=2, label='Training')\n",
    "    ax.plot(xx, yy_v, 'r', lw=2, label='Validation')\n",
    "\n",
    "    ax.set_xlabel('Iterations', fontsize=ftsize)\n",
    "    ax.set_ylabel('average loss', fontsize=ftsize)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, which=\"both\", linestyle='-')\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1686170422923,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "i3tXx6bcNkV4",
    "outputId": "bbd603f0-2858-48c7-8d7d-3e854c54350e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(34, 200)\n",
      "    (pos_embedding): Embedding(61, 200)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x EncoderLayer(\n",
      "        (self_attention): MultiHeadAttention(\n",
      "          (linear_Q): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (linear_K): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (linear_V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (linear_O): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (self_attention_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (feedforward): Feedforward(\n",
      "          (linear_1): Linear(in_features=200, out_features=1024, bias=True)\n",
      "          (linear_2): Linear(in_features=1024, out_features=200, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feedforward_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (tok_embedding): Embedding(28, 200)\n",
      "    (pos_embedding): Embedding(200, 200)\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x DecoderLayer(\n",
      "        (self_attention): MultiHeadAttention(\n",
      "          (linear_Q): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (linear_K): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (linear_V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (linear_O): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (self_attention_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): MultiHeadAttention(\n",
      "          (linear_Q): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (linear_K): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (linear_V): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (linear_O): Linear(in_features=200, out_features=200, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (attention_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (feedforward): Feedforward(\n",
      "          (linear_1): Linear(in_features=200, out_features=1024, bias=True)\n",
      "          (linear_2): Linear(in_features=1024, out_features=200, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feedforward_norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=200, out_features=28, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 5,294,420 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "MAX_SRC_LEN= dloader.SRC_SEQ_LEN\n",
    "INPUT_DIM  = dloader.SRC_VOCAB_SIZE\n",
    "\n",
    "MAX_TRG_LEN= dloader.TGT_SEQ_LEN\n",
    "OUTPUT_DIM = dloader.TGT_VOCAB_SIZE\n",
    "\n",
    "EMB_DIM    = 200\n",
    "\n",
    "ENC_LAYERS = 4\n",
    "ENC_HEADS  = 8\n",
    "ENC_FF_DIM = 1024\n",
    "ENC_DROPOUT= 0.1\n",
    "\n",
    "DEC_LAYERS = 4\n",
    "DEC_HEADS  = 8\n",
    "DEC_FF_DIM = 1024\n",
    "DEC_DROPOUT= 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              EMB_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_FF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              DEVICE, \n",
    "              MAX_SRC_LEN)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              EMB_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_FF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              DEVICE, \n",
    "              MAX_TRG_LEN)\n",
    "\n",
    "PAD_CODE = dloader.PAD\n",
    "SOS_CODE = dloader.SOS\n",
    "EOS_CODE = dloader.EOS\n",
    "\n",
    "model    = Seq2Seq(enc, dec, PAD_CODE, PAD_CODE, DEVICE).to(DEVICE)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        \n",
    "model.apply(initialize_weights)\n",
    "print(model)\n",
    "\n",
    "print(f'The model has {number_of_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1686170433105,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "xvHc_UNHNkV4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "VERSION     = '06-11-23-v1'\n",
    "DICTFILE    = pathname(f'seq2seq_series-{VERSION:s}.pt')\n",
    "LOSSFILE    = pathname(f'seq2seq_losses-{VERSION:s}.txt')\n",
    "\n",
    "os.system(f'rm -rf {LOSSFILE:s}')\n",
    "\n",
    "traces=([], [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "executionInfo": {
     "elapsed": 2491043,
     "status": "ok",
     "timestamp": 1686172942222,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "dSKnr-o3NkV4",
    "outputId": "a3054dd4-7a16-4062-c329-4513e266a453"
   },
   "outputs": [],
   "source": [
    "LOAD        = True\n",
    "TRAIN       = False\n",
    "\n",
    "if LOAD:\n",
    "    # load best model\n",
    "    model.load_state_dict(torch.load(DICTFILE, map_location=torch.device(DEVICE)))\n",
    "\n",
    "if TRAIN:\n",
    "    BATCH_SIZE    = 128\n",
    "    LEARNING_RATE = 5e-4\n",
    "    NITERATIONS   = 20000\n",
    "    STEP          =   100\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_CODE)\n",
    "    \n",
    "    traces    = train(model, optimizer, criterion, dloader,\n",
    "                      niterations=NITERATIONS, \n",
    "                      dictfile=DICTFILE,\n",
    "                      batch_size=BATCH_SIZE, \n",
    "                      pad_code=PAD_CODE, \n",
    "                      traces=traces, \n",
    "                      lossfile=LOSSFILE, \n",
    "                      step=STEP)\n",
    "    \n",
    "    plot_average_loss(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXFWbhNtNkV4"
   },
   "source": [
    "## Testing Model\n",
    "\n",
    "Our test data are tokenized, coded, and bracketed with the `<sos>` and `<eos>` codes. Therefore, the translation steps are as follows:\n",
    "Steps:\n",
    "\n",
    "- convert list of (coded) source tokens, `src`, to the tensor, `src_`, and add a batch dimension to it so that the source is of the correct shape, namely, `[batch size, src seq len]` with `batch size = 1`;\n",
    "- create the source mask (though, strictly speaking, this is not needed here since, unlike the training and validation data, the test data are unpadded);\n",
    "- feed the source and its mask into the encoder;\n",
    "- create a list to hold the predicted tokens initialized with the `<sos>` token;\n",
    "- while we have not hit a maximum length\n",
    "  - convert the current output sentence prediction into a tensor with a batch dimension\n",
    "  - create a target sentence mask\n",
    "  - place the current output, encoder output and both masks into the decoder\n",
    "  - get next output token prediction from decoder along with attention\n",
    "  - add prediction to current output sentence prediction\n",
    "  - break if the prediction was an `<eos>` token\n",
    "- convert the output sentence from indexes to tokens\n",
    "- return the output sentence (with the `<sos>` token removed) and the attention from the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1686168194747,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "y3-IQ2AdNkV4"
   },
   "outputs": [],
   "source": [
    "def translate(src, model, \n",
    "              max_len=256, \n",
    "              sos=SOS_CODE, \n",
    "              device=DEVICE, \n",
    "              threshold=0.50):\n",
    "    \n",
    "    from copy import copy\n",
    "    \n",
    "    def execute(trg, src_, src_mask):\n",
    "        \n",
    "        trg_ = torch.tensor(trg).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(trg_, src_, trg_mask, src_mask)\n",
    "            # [batch size, trg seq len, trg vocab size]\n",
    "            # assume batch_size = 1 since src is a single sequence\n",
    "            # apply softmax to the last dimension (the default) of \n",
    "            # the last target position\n",
    "            probs  = torch.softmax(output[:,-1], dim=1)\n",
    "            \n",
    "        # for each output position return the 2 token codes with \n",
    "        # the largest probabilities\n",
    "        token_probs, token_codes = torch.topk(probs, k=2)\n",
    "        token_probs = token_probs.t()\n",
    "        token_codes = token_codes.t()\n",
    "\n",
    "        token_code0 = token_codes[0,-1].item()\n",
    "        token_code1 = token_codes[1,-1].item()\n",
    "        \n",
    "        token_prob0 = token_probs[0,-1].item()\n",
    "        token_prob1 = token_probs[1,-1].item()\n",
    "\n",
    "        return token_code0, token_code1, token_prob0, token_prob1\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    src_ = torch.LongTensor(src).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        src_ = model.encoder(src_, src_mask)\n",
    "\n",
    "    trg0 = [sos]\n",
    "    trg1 = None\n",
    "    \n",
    "    prb0 = 1.0\n",
    "    prb1 = 1.0\n",
    "    \n",
    "    first= True\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        \n",
    "        single_sequence = trg1 == None\n",
    "        double_sequence = not single_sequence\n",
    "        \n",
    "        code0, code1, prob0, prob1 = execute(trg0, src_, src_mask)\n",
    "\n",
    "        code = code0\n",
    "            \n",
    "        trg0.append(code)\n",
    "            \n",
    "        if code == EOS_CODE:\n",
    "            break\n",
    "            \n",
    "    return trg0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translate():\n",
    "    def __init__(self, model, max_len=256, sos=SOS_CODE, device=DEVICE):\n",
    "        self.model   = model\n",
    "        self.max_len = max_len\n",
    "        self.sos     = sos\n",
    "        self.device  = device\n",
    "        \n",
    "    def exec_model(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self, src, beam_size=2):\n",
    "        model.eval()\n",
    "\n",
    "        src_ = torch.LongTensor(src).unsqueeze(0).to(self.device)\n",
    "    \n",
    "        src_mask = model.make_src_mask(src_)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            src_ = model.encoder(src_, src_mask)\n",
    "\n",
    "        trgs  = [[sos] for _ in range(beam_size)]\n",
    "        vals  = [[1] for _ in range(beam_size)]\n",
    "        \n",
    "        for i in range(max_len):\n",
    "\n",
    "            for trg in trgs:\n",
    "                trg_ = torch.tensor(trg).unsqueeze(0).to(device)\n",
    "\n",
    "                trg_mask = model.make_trg_mask(trg_)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model.decoder(trg_, src_, trg_mask, src_mask)\n",
    "                    # [batch size, trg seq len, trg vocab size]\n",
    "\n",
    "                value, index = torch.topk(output[:,-1, :], beam_size, dim=2)\n",
    "                vals = []\n",
    "            # return the code (i.e., the token's ordinal value in the\n",
    "            # target vocabulary) of the last token in the current target\n",
    "            # sequence with the largest output value.\n",
    "            token_code = output.argmax(dim=2)[:,-1].item()\n",
    "\n",
    "            trg.append(token_code)\n",
    "\n",
    "            if token_code == EOS_CODE:\n",
    "                break\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150384,
     "status": "ok",
     "timestamp": 1686173218763,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "fFSdM2NvNkV5",
    "outputId": "f4fa4977-7bdc-4959-93b7-933af1237402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     399\taccuracy:    0.868 +/- 0.017\n"
     ]
    }
   ],
   "source": [
    "srcs, tgts = dloader.test_data\n",
    "MAX_LEN    = dloader.TGT_SEQ_LEN\n",
    "PRINT_MISTAKES = False\n",
    "\n",
    "# load best model\n",
    "model.load_state_dict(torch.load(DICTFILE, map_location=torch.device(DEVICE)))\n",
    "\n",
    "N = 400\n",
    "M = 0\n",
    "F = 0.0\n",
    "\n",
    "for i, (src, tgt) in enumerate(zip(srcs[:N], tgts[:N])):   \n",
    "\n",
    "    src_ = stringify(src[1:-1], dloader.src_code2token)\n",
    "    \n",
    "    tgt_ = stringify(tgt[1:-1], dloader.tgt_code2token)\n",
    "    \n",
    "    out  = translate(src, model, \n",
    "                               max_len=MAX_LEN, \n",
    "                               sos=SOS_CODE, \n",
    "                               device=DEVICE)\n",
    "    \n",
    "    out_ = stringify(out[1:-1], dloader.tgt_code2token)\n",
    "    \n",
    "    tgt_ = tgt_.replace('<pad>','')\n",
    "\n",
    "    if out_ == tgt_:\n",
    "        M += 1\n",
    "        F = M / (i+1)\n",
    "    else:\n",
    "        if PRINT_MISTAKES:\n",
    "            print()\n",
    "            print(tgt_)\n",
    "            print()\n",
    "            print(out_)\n",
    "            print()\n",
    "            print('-'*91)\n",
    "\n",
    "    print(f'\\r{i:8d}\\taccuracy: {F:8.3f}', end='')\n",
    "\n",
    "dF = math.sqrt(F*(1-F)/N)\n",
    "print(f'\\r{i:8d}\\taccuracy: {F:8.3f} +/- {dF:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaK2Rb1KwxSR"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 1427,
     "status": "ok",
     "timestamp": 1686173496766,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "xqsxpTU3NkV6"
   },
   "outputs": [],
   "source": [
    "def compute_loss_from_lists(x, t, model, avloss, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if type(x) == type([]):\n",
    "        x = torch.tensor(x)\n",
    "        t = torch.tensor(t)\n",
    "\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    t = t.unsqueeze(0).to(device)\n",
    "\n",
    "    # slice off EOS token from targets\n",
    "    y = model(x, t[:,:-1])\n",
    "    # [batch_size, trg_seq_len, trg_vocab_size]\n",
    "\n",
    "    trg_vocab_size = y.shape[-1]\n",
    "\n",
    "    y_out = y.reshape(-1, trg_vocab_size)\n",
    "    # [batch_size * tgt_seq_len, tgt_vocab_size]\n",
    "\n",
    "    # slice of SOS token from targets\n",
    "    t_out = t[:, 1:].reshape(-1)\n",
    "    # [batch_size * tgt_seq_len]\n",
    "\n",
    "    loss  = avloss(y_out, t_out).mean().item()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5327,
     "status": "ok",
     "timestamp": 1686173504863,
     "user": {
      "displayName": "Harrison Prosper",
      "userId": "06677011918885244894"
     },
     "user_tz": -120
    },
    "id": "GTxgVerMXyiT",
    "outputId": "2ff0553b-159a-4859-ed1c-d3c10227787e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<loss>:     0.0169\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_CODE)\n",
    "N = 400\n",
    "aloss = 0.0\n",
    "for i, (src, tgt) in enumerate(zip(srcs[:N], tgts[:N])):\n",
    "    aloss += compute_loss_from_lists(src, tgt, model, criterion, DEVICE)\n",
    "aloss /= N\n",
    "print(f'<loss>: {aloss:10.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
